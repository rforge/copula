\documentclass{article}
%
\usepackage{fullpage}% save trees ;-)
\usepackage{Sweave}
\usepackage{myVignette}% includes Marius' definitions
\usepackage[authoryear,round]{natbib}
\setlength{\bibsep}{0pt}
\bibliographystyle{plainnat}
\hyphenation{Ar-chi-me-dean}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=7,height=4,strip.white=TRUE,keep.source=TRUE}
%
\begin{document}
\thispagestyle{plain}
\begin{center}
	{\bfseries\LARGE\mytitle\par}
	\bigskip
	{\Large\myauthorone\footnote{\mycontactone},\ \myauthortwo\footnote{\mycontacttwo}\par
	\bigskip
	\mydate\par}
\end{center}
\par\bigskip
\begin{abstract}
% \emph{\Large Why should you want to work with this package and what
%   does it do for you?}
  \noindent Sampling algorithms for multivariate distributions are indispensable for many applications in the areas of statistics and finance. By Sklar's Theorem, it suffices to sample the common dependence structure, given by a copula, and transform the obtained variates to the correct margins with the corresponding univariate distributions. Therefore, the sampling problem boils down to sampling the copula under consideration. Besides elliptical copulas, Archimedean copulas are among the most famous classes of copulas. These copulas can be generalized to the class of nested Archimedean copulas, which allows for asymmetries and is therefore more flexible.
  \par\noindent
  The \pkg{nacopula} package provides procedures for computing function values, characteristics such as Kendall's tau and tail dependence coefficients, as well as sampling algorithms for generating random variates from nested Archimedean copulas.
\end{abstract}
%% Note: These are explained in '?RweaveLatex' :
<<preliminaries, echo=FALSE>>=
options(width=75)
@
\section{Introduction}\label{sec:Intro}
\subsection{Archimedean copulas and their properties}
  \textit{Copulas} are distribution functions with standard uniform univariate margins. An \textit{Archimedean generator}, or simply \textit{generator}, is a continuous, decreasing function $\psi:[0,\infty]\to[0,1]$ which satisfies $\psi(0)=1$, $\psi(\infty):=\lim_{t\to\infty}\psi(t)=0$, and which is strictly decreasing on $[0,\inf\{t:\psi(t)=0\}]$. A $d$-dimensional copula $C$ is called \textit{Archimedean} if it permits the representation
	\begin{align}
		C(\bm{u})=C(\bm{u};\psi):=\psi(\psii(u_1)+\dots+\psii(u_d)),\ \bm{u}\in I^d,\label{16}
	\end{align}
	for some generator $\psi$ with inverse $\psii:[0,1]\to[0,\infty]$, where $\psii(0):=\inf\{t:\psi(t)=0\}$. \citet{mcneilneslehova2009} show that a generator defines an Archimedean copula if and only if $\psi$ is $d$\textit{-monotone}, i.e., $\psi$ is continuous on $[0,\infty]$, admits derivatives up to the order $d-2$ satisfying $\smash[t]{(-1)^k\frac{d^k}{dt^k}\psi(t)\ge 0}$ for all $k\in\{0,\dots,d-2\}$, $t\in(0,\infty)$, and $\smash[t]{(-1)^{d-2}\frac{d^{d-2}}{dt^{d-2}}\psi(t)}$ is decreasing and convex on $(0,\infty)$. Throughout this work we assume $\psi$ to be \textit{completely monotone}, i.e., $\psi$ is continuous on $[0,\infty]$ and $\smash[t]{(-1)^k\frac{d^k}{dt^k}\psi(t)\ge 0}$ for all $k\in\IN_{0}$, $t\in(0,\infty)$, so that $\psi$ is the Laplace-Stieltjes transform of a distribution function $F$ on the positive real line, i.e., $\psi=\LS[F]$, see Bernstein's Theorem in \citet[p.\ 439]{feller1971}. 
	\par
	If we know how to efficiently sample $F=\LSi[\psi]$, the following algorithm by \citet{marshallolkin1988} easily generates vectors of random variates following the Archimedean copula generated by $\psi$ in all dimensions $d$. As usual, ``i.i.d.'' means ``independent and identically distributed''.
	\begin{algorithm}[\citet{marshallolkin1988}]\label{21}
		\myskip
		\begin{enumerate}[label=(\arabic*),leftmargin=*,align=left,itemsep=0mm,topsep=1mm]
			\item Sample $V\sim F:=\LSi[\psi]$.
			\item Sample i.i.d.\ $X_j\sim\U[0,1]$, $j\in\{1,\dots,d\}$, independently of $V$.
			\item Return $\bm{U}$, where $U_j:=\psi(-\log(X_j)/V)$, $j\in\{1,\dots,d\}$.
		\end{enumerate}
	\end{algorithm}
	It is often desirable to measure the degree of dependence between random variables by a real number. Such measures are referred to as \textit{measures of association} and are usually studied for pairs of random variables. One such measure is \textit{Kendall's tau}, defined by
	\begin{align*}
		\tau:=\IE[\sgn((X_1-X_1^\prime)(X_2-X_2^\prime))],
	\end{align*}
	where $(X_1,X_2)^T$ is a vector of two continuously distributed random variables, $(X_1^\prime,X_2^\prime)^T$ is an i.i.d.\ copy of $(X_1,X_2)^T$, and $\sgn(x):=\I_{(0,\infty)}(x)-\I_{(-\infty,0)}(x)$. Kendall's tau is often used to measure the overall degree of dependence between two random variables. To be more precise, it is a measure of concordance, see \citet{scarsini1984}, and therefore, informally, measures the likelihood, as a number in $[-1,1]$, with which large values of one variable are associated with large values of the other. It can be expressed in terms of the copula $C$ corresponding to $(X_1,X_2)^T$ by
	\begin{align*}
	  \tau=4\int_{I^2}C(\bm{u})\,dC(\bm{u})-1,
	\end{align*}
	see \citet[p.\ 161]{nelsen2007}. If $C$ is a bivariate Archimedean copula generated by a twice continuously differentiable generator $\psi$ with $\psi(t)>0$, $t\in[0,\infty)$, Kendall's tau can be represented in semi-closed form as
	\begin{align*}
  	\tau=4\int_0^1\frac{\psii(t)}{(\psii(t))^\prime}\,dt+1=1-4\int_0^\infty t(\psi^\prime(t))^2\,dt,
	\end{align*}
	see \citet[p.\ 91]{joe1997}.
	\par
	Tail dependence measures the probability that one random variable takes on values in its tail, given the other one takes on values in its tail. To be more precise, if $X_j\sim F_j$, $j\in\{1,2\}$, are continuously distributed random variables, the \textit{lower tail-dependence coefficient}, respectively the \textit{upper tail-dependence coefficient}, of $X_1$ and $X_2$ are defined as
	\begin{align*}
		\lambda_l:=\lim_{t\downarrow0}\IP(X_2\le F_2^-(t)\,\vert\,X_1\le F_1^-(t)),\quad\lambda_u:=\lim_{t\uparrow1}\IP(X_2>F_2^-(t)\,\vert\,X_1>F_1^-(t)),
	\end{align*}
	provided that the limits exist. These measures of association can also be expressed in terms of the copula $C$ corresponding to $(X_1,X_2)^T$ by
	\begin{align*}
		\lambda_l=\lim_{t\downarrow0}\frac{C(t,t)}{t},\quad\lambda_u=\lim_{t\uparrow1}\frac{1-2t+C(t,t)}{1-t},
	\end{align*}
	where the appearing limits exist if and only if $\lambda_l$ and $\lambda_u$ exist. If $C$ is a bivariate Archimedean copula generated by $\psi$ with $\psi(t)>0$, $t\in[0,\infty)$, then
	\begin{align*}
		\lambda_l=\lim_{t\to\infty}\frac{\psi(2t)}{\psi(t)}=2\lim_{t\to\infty}\frac{\psi^\prime(2t)}{\psi^\prime(t)},\quad\lambda_u=2-\lim_{t\downarrow0}\frac{1-\psi(2t)}{1-\psi(t)}=2-2\lim_{t\downarrow0}\frac{\psi^\prime(2t)}{\psi^\prime(t)},
	\end{align*}
	where the equalities involving derivatives are obtained by l'H\^opital's rule and therefore the corresponding assumptions are required to hold.
\subsection{Nested Archimedean copulas}
  In contrast to elliptical copulas, Archimedean copulas are not restricted to radial symmetry, which implies that they can capture different tail dependencies, i.e., $\lambda_l\neq\lambda_u$. Further, they are given explicitly, see (\ref{16}), hence function evaluations are usually easy to establish. Note that the functional symmetry of Archimedean copulas implies that all margins of the same dimension are equal. For modeling purposes, this becomes an increasingly strong assumption in the dimension. Asymmetries, thus more realistic dependencies, can be modeled by a hierarchical structure of Archimedean copulas, obtained by plugging in Archimedean copulas into each other. This construction leads to nested Archimedean copulas.
	\par
	A $d$-dimensional copula $C$ is called \textit{nested Archimedean} if it is an Archimedean copula with arguments possibly replaced by other nested Archimedean copulas. If $C$ is given recursively by (\ref{16}) for $d=2$ and
	\begin{align}
		C(\bm{u};\psi_0,\dots,\psi_{d-2}):=\psi_0\bigl(\psiis{0}(u_1)+\psiis{0}(C(u_2,\dots,u_d;\psi_{1},\dots,\psi_{d-2}))\bigr),\ \bm{u}\in I^d,\label{18}
	\end{align}
	for $d\ge3$, $C$ is called \textit{fully-nested Archimedean copula} with $d-1$ \textit{nesting levels} or \textit{hierarchies}. Fully-nested Archimedean copulas may also be obtained by nesting in one of the other arguments, not necessarily in the last one as done in (\ref{18}). Otherwise, $C$ is called \textit{partially-nested Archimedean copula}. 
	\par
	In order for (\ref{18}) being indeed a proper copula, \citet[p.\ 88]{joe1997} and \citet{mcneil2008} present the sufficient \textit{nesting condition} that $\psiis{i}\circ\psi_{j}$ have completely monotone derivatives for all nodes appearing in a nested Archimedean copula. This condition can be derived from a mixture representation of $C$ based on the \textit{outer distribution function} $F_0:=\LSi[\psi_0]$ corresponding to the \textit{outer generator} $\psi_0$ and the \textit{inner distribution functions} $F_{ij}:=\LSi[\psi_{ij}(\cdot\,;V_0)]$ corresponding to the \textit{inner generators} $\psi_{ij}(t;x):=\exp\bigl(-x\psiis{i}(\psi_{j}(t))\bigr)$, $x>0$, appearing in the Archimedean structure.
	\par
	Nested Archimedean copulas combine the nice properties of Archimedean copulas as mentioned above with the ability to model asymmetries. They are therefore of interest from both a theoretical point of view and for applications. Since the bivariate margins of a nested Archimedean copula are Archimedean copulas, the formulas for Kendall's tau and the tail-dependence coefficients as given above readily take over.
	\par
	For sampling fully-nested Archimedean copulas of Type (\ref{18}), \citet{mcneil2008} presented the following algorithm.
	\begin{algorithm}[\citet{mcneil2008}]
		\myskip
		\begin{enumerate}[label=(\arabic*),leftmargin=*,align=left,itemsep=0mm,topsep=1mm]
			\item Sample $V_0\sim F_0:=\LSi[\psi_0]$.
			\item\label{201} Sample $X_1\sim\U[0,1]$ independently of $V_0$ and $(X_2,\dots,X_d)^T\sim C(u_2,\dots,u_d;\psi_{01}(\cdot\,;$ $V_0),\dots,\psi_{0\hspace{0.2mm}d-2}(\cdot\,;V_0))$ independently of $X_1$.
			\item Return $\bm{U}$, where $U_j:=\psi_0(-\log(X_j)/V_0)$, $j\in\{1,\dots,d\}$.
		\end{enumerate}
	\end{algorithm}
	Similar to this algorithm for sampling (\ref{18}), a general procedure for sampling all kinds of nested Archimedean copulas can be derived. The idea is to first draw $V_0\sim F_0$ and then to descend downwards in the nested structure and sample the appearing nested Archimedean copulas there. On the lowest level, this involves sampling an Archimedean copula, which can be achieved with the Marshall-Olkin algorithm. Note that for sampling the nested Archimedean copulas appearing in Step \ref{201} of McNeil's algorithm when all generators involved belong to the same parametric family, it suffices to know how to sample $F_{01}=\LSi[\psi_{01}(\cdot\,;V_0)]$, $V_0>0$, as all inner distribution functions $F_{ij}$ take the same form as $F_{01}$, only the parameters might differ. Hence, the main ingredients for sampling nested Archimedean copulas are
	\begin{align*}
		V_0\sim F_0=\LSi[\psi_0]\ \text{and}\ V_{01}\sim F_{01}=\LSi[\psi_{01}(\cdot\,;V_0)].
	\end{align*}
	For sampling strategies for these two distributions for many known Archimedean generators, see \citet{hofert2008}, \citet{hofert2010a}, and \citet{hofert2010c}.
\section{The implemented Archimedean families}
  Among the most widely used one-parametric Archimedean families are the ones of Ali-Mikhail-Haq (``A''), Clayton (``C''), Frank (``F''), Gumbel (``G''), and Joe (``J''), see, e.g., \citet[pp.\ 116]{nelsen2007}. For simplicity, we choose a slightly different generator for Clayton's family. Tables \ref{table1}, \ref{table2}, and \ref{table3} list these generators with corresponding inverses, Kendall's tau, tail-dependence coefficients, conditions on two parameters $\vt_0$ and $\vt_1$ such that $\psi_{01}$ fulfills the sufficient nesting condition, and instructions for sampling $V_0$ and $V_{01}$.
	\begin{table}[htbp] 
		\centering
		\begin{tabularx}{\textwidth}{@{\extracolsep{\fill}}c@{\extracolsep{-1pt}}c@{\extracolsep{3pt}}c@{\extracolsep{8pt}}c@{\extracolsep{-30pt}}c}
			\toprule
			\multicolumn{1}{c}{Family}&\multicolumn{1}{c}{$\vt$}&\multicolumn{1}{c}{$\psi(t)$}&\multicolumn{1}{c}{$\psii(t)$}&\multicolumn{1}{c}{$V\sim F=\LSi[\psi]$}\\
			\midrule
			A&$[0,1)$&$(1-\vt)/(\exp(t)-\vt)$&$\log((1-\vt(1-t))/t)$&$\Geo(1-\vt)$\\
			C&$(0,\infty)$&$(1+t)^{-{1/\vt}}$&$t^{-\vt}-1$&$\Gamma(1/\vt,1)$\\
			F&$(0,\infty)$&$-\log\bigl(1-(1-e^{-\vt})\exp(-t)\bigr)/\vt$&$-\log((1-\exp(-\vt t))/(1-e^{-\vt}))$&$\Log(1-e^{-\vt})$\\
			G&$[1,\infty)$&$\exp(-t^{1/\vt})$&$(-\log(t))^{\vt}$&$\S(1/\vt,1,\cos^\vt(\pi/(2\vt)),\I_{\{\vt=1\}};1)$\\
			J&$[1,\infty)$&$1-(1-\exp(-t))^{1/\vt}$&$-\log(1-(1-t)^{\vt})$&$\binom{1/\vt}{k}(-1)^{k-1},\ k\in\IN$\\
      % 12&$[1,\infty)$&$(1+t^{1/\vt})^{-1}$&$\S(1/\vt,1,\cos^{\vt}(\pi/(2\vt)),\I_{\{\vt=1\}};1)\Exp(1)^\vt$\\
      % 13&$[1,\infty)$&$\exp(1-(1+t)^{1/\vt})$&$\tS(1/\vt,1,\cos^{\vt}(\pi/(2\vt)),\I_{\{\vt=1\}},\I_{\{\vt\neq1\}};1)$\\
      % 14&$[1,\infty)$&$(1+t^{1/\vt})^{-\vt}$&$\S(1/\vt,1,\cos^{\vt}(\pi/(2\vt)),\I_{\{\vt=1\}};1)\Gamma(\vt,1)^\vt$\\
      % 19&$(0,\infty)$&$\vt/\log(t+e^\vt)$&$\Gamma(\Exp(1)/\vt,e^{\vt})$\\
      % 20&$(0,\infty)$&$\log^{-{1/\vt}}(t+e)$&$\Gamma(\Gamma(1/\vt,1),e)$\\
			\bottomrule 
		\end{tabularx}
		\caption{Commonly used one-parameter Archimedean generators.}
		\label{table1}
	\end{table}
	\par
	The last column of Table \ref{table1} contains the distributions $F$ to be sampled for the first step of the Marshall-Olkin algorithm. Note that this is the same distribution as $F_0$ which has to be drawn for sampling a nested Archimedean copulas with McNeil's algorithm, simply replace $\vt$ by $\vt_0$. For the family of Ali-Mikhail-Haq, $\Geo(p)$ denotes a \textit{geometric distribution} with success probability $p\in(0,1]$ and mass function $p_k=p(1-p)^{k-1}$ at $k\in\IN$. For Clayton's family, $\Gamma(\alpha,\beta)$ denotes the \textit{gamma distribution} with shape $\alpha\in(0,\infty)$, scale $\beta\in(0,\infty)$, and density $f(x)=\beta^\alpha x^{\alpha-1}\exp(-\beta x)/\Gamma(\alpha)$, $x\in(0,\infty)$. For the family of Frank, $\Log(p)$ denotes a \textit{logarithmic distribution} with parameter $p\in(0,1)$ and mass function $p_k=p^k/(-k\log(1-p))$ at $k\in\IN$. For sampling logarithmic distributions, the sampling algorithm ``LK'' of \citet{kemp1981} can be applied. For Gumbel's family, $F$ corresponds to a stable distribution, see \citet[p.\ 8]{nolan2009} for the parameterization. An efficient algorithm for sampling stable distributions is given in \citet{chambersmallowsstuck1976}. For the family of Joe, $p_k=\binom{1/\vt}{k}(-1)^{k-1}$ at $k\in\IN$, with $\vt\in[1,\infty)$. This distribution can be sampled with an algorithm found in \citet{hofert2010a}.
	\begin{table}[htbp] 
		\centering
		\begin{tabularx}{\textwidth}{@{\extracolsep{\fill}}ccccc}
			\toprule
				\multicolumn{1}{c}{Family}&\multicolumn{1}{c}{$\vt$}&\multicolumn{1}{c}{$\tau$}&\multicolumn{1}{c}{$\lambda_l$}&\multicolumn{1}{c}{$\lambda_u$}\\
			\midrule
			A&$[0,1)$&$1-2(\vt+(1-\vt)^2\log(1-\vt))/(3\vt^2)$&0&0\\
			C&$(0,\infty)$&$\vt/(\vt+2)$&$2^{-1/\vt}$&0\\
			F&$(0,\infty)$&$1+4(D_1(\vt)-1)/\vt$&0&0\\
			G&$[1,\infty)$&$(\vt-1)/\vt$&0&$2-2^{1/\vt}$\\
			J&$[1,\infty)$&$\sum_{k=1}^\infty 1/(k(\vt k+2)(\vt(k-1)+2))$&0&$2-2^{1/\vt}$\\
      % 12&$[1,\infty)$&$1-2/(3\vt)$&$2^{-1/\vt}$&$2-2^{1/\vt}$\\
      % 13&$[1,\infty)$&$1-(3-2^{\vt}e^2\Gamma(2-\vt,2))/\vt$&0&0\\
      % 14&$[1,\infty)$&$1-2/(1+2\vt)$&$1/2$&$2-2^{1/\vt}$\\
      % 19&$(0,\infty)$&$1/3+2\vt(1-\vt e^\vt E_1(\vt))/3$&1&0\\
      % 20&$(0,\infty)$&$-$&1&0\\
			\bottomrule 
		\end{tabularx}
		\caption{Kendall's tau and tail-dependence coefficients for commonly used one-parameter Archimedean generators.}
		\label{table2}
	\end{table}
  \par
  Table \ref{table2} contains formulas for the concordance measure Kendall's tau and the lower and upper tail-dependence coefficient. For Frank's family, $D_1(\vt):=\int_0^\vt t/(\exp(t)-1)\,dt/\vt$ denotes the \textit{Debye function of order one}.
	\begin{table}[htbp] 
		\centering
		\begin{tabularx}{\textwidth}{@{\extracolsep{\fill}}ccc}
			\toprule
			\multicolumn{1}{c}{Family}&\multicolumn{1}{c}{nesting condition}&$V_{01}\sim F_{01}=\LSi[\psi_{01}(\cdot\,;V_0)]$\\ 
			\midrule
			A&$\vt_0\le\vt_1$&$V_0+X$, $X\sim\NB(V_0,(1-\vt_1)/(1-\vt_0))$\\
			C&$\vt_0\le\vt_1$&$\tS(\vt_0/\vt_1,1,(\cos(\pi\vt_0/(2\vt_1))V_0)^{\vt_1/\vt_0},V_0\I_{\{\vt_0=\vt_1\}},\I_{\{\vt_0\neq\vt_1\}};1)$\\
			F&$\vt_0\le\vt_1$&$\sum_{k=1}^{V_0}V_k$\\
			G&$\vt_0\le\vt_1$&$\S(\vt_0/\vt_1,1,(\cos(\pi\vt_0/(2\vt_1))V_{0})^{\vt_1/\vt_0},\I_{\{\vt_0=\vt_1\}};1)$\\
			J&$\vt_0\le\vt_1$&$\sum_{k=1}^{V_0}V_k$\\
      % 12&$\vt_0\le\vt_1$&$\S(\vt_0/\vt_1,1,(\cos(\pi\vt_0/(2\vt_1))V_{0})^{\vt_1/\vt_0},\I_{\{\vt_0=\vt_1\}};1)$\\
      % 13&$\vt_0\le\vt_1$&$\tS(\vt_0/\vt_1,1,(\cos(\pi\vt_0/(2\vt_1))V_0)^{\vt_1/\vt_0},V_0\I_{\{\vt_0=\vt_1\}},\I_{\{\vt_0\neq\vt_1\}};1)$\\
      % 14&$-$&$-$\\
      % 19&$\vt_0\le\vt_1$&$\tS(\vt_0/\vt_1,1,(\cos(\pi\vt_0/(2\vt_1))V_0)^{\vt_1/\vt_0},V_0\I_{\{\vt_0=\vt_1\}},e^{\vt_1}\I_{\{\vt_0\neq\vt_1\}};1)$\\
      % 20&$\vt_0\le\vt_1$&$-$\\
			\bottomrule
		\end{tabularx}
		\caption{Nesting conditions and inverse Laplace-Stieltjes transforms corresponding to the inner generator $\psi_{01}(\cdot\,;V_0)$.}
		\label{table3}
	\end{table}
  \par
  Table \ref{table3} contains instructions for sampling $V_{01}$ as required for sampling nested Archimedean copulas with McNeil's algorithm. The distribution of $V_{01}$ for Gumbel's family is addressed in \citet{mcneil2008}, all other results can be found in \citet{hofert2010a}. For Ali-Mikhail-Haq's family, $\NB(r,p)$, $r\in(0,\infty)$, $p\in(0,1)$, denotes the negative binomial distribution with mass function $p_k=\binom{k+r-1}{r-1}p^r(1-p)^k$, $k\in\IN_0$. For Clayton's family, $V_{01}$ follows an \textit{exponentially tilted stable distribution} of the form $\tS(\alpha,1,(\cos(\alpha\pi/2)V_0)^{1/\alpha},V_0\I_{\{\alpha=1\}},h\I_{\{\alpha\neq1\}};1)$ with Laplace-Stieltjes transform $\tilde{\psi}(t)=\exp\bigl(-V_0((h+t)^{\alpha}-h^{\alpha})\bigr)$, see \citet{hofert2010a} for more details and an efficient sampling algorithm for these distributions. For the families of Frank and Joe, $V_{01}$ can be sampled as a $V_0$-fold sum of i.i.d.\ random variables, see \citet{hofert2010a} for sampling algorithms for the distributions of the summands $V_k$, $k\in\{1,\dots,V_0\}$. 
\section{Working with nested Archimedean copulas}

  todo: include details about the procedure for generating stable random numbers
  \par
  todo: introduce classes, technical stuff
  \par
  todo: evaluating, sampling, kendall's tau, plots (?) => examples

% The most automatic way to use the \pkg{Matrix} package is via the
% \Rfun{Matrix} function which is very similar to the standard \RR\ function
% \Rfun{matrix},
% @
% <<ex1>>=
%
% @ %def
% Such a matrix can be appended to (using \Rfun{cBind} or \Rfun{rBind}
% with capital ``B'') or indexed,
% <<ex2>>=
% (M2 <- cBind(-1, M))
% M[2, 1]
% M[4, ]
% @
% where the last two statements show customary matrix indexing, returning a
% simple numeric vector each\footnote{because there's an additional default
%   argument to indexing, \code{drop = TRUE}.  If you add
%   \hbox{``\code{\ ,\ drop = FALSE}\ ''} you will get submatrices instead of
%   simple vectors.}.
% We assign 0 to some columns and rows to ``sparsify'' it, and some \code{NA}s
% (typically ``missing values'' in data analysis) in order to demonstrate how
% they are dealt with; note how we can \emph{``subassign''} as usual,
% for classical \RR{} matrices (i.e., single entries or whole slices at once),
% @
% <<set0>>=
% M2[, c(2,4:6)] <- 0
% M2[2, ] <- 0
% M2 <- rBind(0, M2, 0)
% M2[1:2,2] <- M2[3,4:5] <- NA
% @
% and then coerce it to a sparse matrix,
% @
% <<asSparse>>=
% sM <- as(M2, "sparseMatrix")
% 10 * sM
% identical(sM * 2, sM + sM)
% is(sM / 10  +  M2 %/% 2, "sparseMatrix")
% @ %def
% where the last three calls show that multiplication by a scalar keeps
% sparcity, as does other arithmetic, but addition to a ``dense'' object does not,
% as you might have expected after some thought about ``sensible'' behavior:
% @
% <<add1>>=
% sM + 10
% @ %def
%
% Operations on our classed matrices include
% (componentwise) arithmetic ($+$, $-$, $*$, $/$, etc) as partly seen above,
% comparison ($>$, $\le$, etc), e.g.,
% <<Comp1>>=
% Mg2 <- (sM > 2)
% Mg2
% @
% returning a logical sparse matrix.  When interested in the internal
% \textbf{str}ucture, \Rfun{str} comes handy, and we have been using it
% ourselves more regulary than \Rfun{print}ing (or \Rfun{show}ing as it
% happens) our matrices; alternatively, \Rfun{summary} gives output similar
% to Matlab's printing of sparse matrices.
% @
% <<str_mat>>=
% str(Mg2)
% summary(Mg2)
% @
% As you see from both of these, \code{Mg2} contains ``extra zero'' (here
% \code{FALSE}) entries; such sparse matrices may be created for different reasons,
% and you can use \Rfun{drop0} to remove (``drop'') these extra zeros.  This
% should \emph{never} matter for functionality, and does not even show
% differently for logical sparse matrices, but the internal structure is more
% compact:
% <<drop0>>=
% Mg2 <- drop0(Mg2)
% str(Mg2@x) # length 13, was 16
% @
% For large sparse matrices, visualization (of the sparsity pattern) is important,
% and we provide \Rfun{image} methods for that, e.g.,
% <<image,fig=TRUE>>=
% data(CAex)
% print(image(CAex, main = "image(CAex)")) # print(.) needed for Sweave
% @
%
% \smallskip
%
% Further, i.e., in addition to the above implicitly mentioned \code{"Ops"} operators
% (\code{+}, \code{*},\dots, \code{<=},\code{>},\dots, \code{\&} which all
% work with our matrices, notably in conjunction with scalars and traditional
% matrices), the \code{"Math"}-operations (such as \Rfun{exp}, \Rfun{sin} or \Rfun{gamma})
% and \code{"Math2"} (\Rfun{round} etc) and the \code{"Summary"} group of
% functions, \Rfun{min}, \Rfun{range}, \Rfun{sum}, all work on our matrices
% as they should.  Note that all these are implemented via so called
% \emph{group methods}, see e.g., \code{?Arith} in \RR.
% The intention is that sparse matrices remain sparse whenever sensible,
% given the matrix \emph{classes} and operators involved,
% but not content specifically. E.g.,  <sparse> + <dense> gives <dense> even
% for the rare cases where it would be advantageous to get a <sparse>
% result.
%
% These classed matrices can be ``indexed'' (more technically ``subset'') as
% traditional \Slang{} (and hence \RR) matrices, as partly seen above. This also includes the idiom
% \code{M [ M \myOp{\mathit{op}} \myOp{\mathrm{num}}~]}
% which returns simple vectors,
% @
% <<sub_logi>>=
% sM[sM > 2]
% sml <- sM[sM <= 2]
% sml
% @ %def
% and \emph{``subassign''}ment similarly works in the same generality as for
% traditional \Slang{} matrices.
% %% We have seen that already above!
%
% %% This was the 2005 - Introduction vignette's first section:
% \subsection{\pkg{Matrix} package for numerical linear algebra}
% \label{ssec:intro-linalg}
%
% Linear algebra is at the core of many statistical computing techniques
% and, from its inception, the \Slang{} has supported numerical linear
% algebra via a matrix data type and several functions and operators,
% such as \code{\%*\%}, \code{qr}, \code{chol}, and \code{solve}.
% %%
% Initially the numerical linear algebra functions in \RR{} called
% underlying Fortran routines from the Linpack~\citep{Linpack} and
% Eispack~\citep{Eispack} libraries but over the years most of these
% functions have been switched to use routines from the
% Lapack~\citep{Lapack} library which is the state-of-the-art implementation
% of numerical dense linear algebra.
% %%
% Furthermore, \RR{} can be configured to
% use accelerated BLAS (Basic Linear Algebra Subroutines), such as those
% from the Atlas~\citep{Atlas} project or other ones, see the \RR~manual
% ``Installation and Administration''.
%
% Lapack provides routines for operating on several special forms of
% matrices, such as triangular matrices and symmetric matrices.
% Furthermore, matrix decompositions like the QR decompositions produce
% multiple output components that should be regarded as parts of a
% single object.  There is some support in \RR{} for operations on special
% forms of matrices (e.g. the \code{backsolve}, \code{forwardsolve} and
% \code{chol2inv} functions) and for special structures (e.g. a QR
% structure is implicitly defined as a list by the \code{qr},
% \code{qr.qy}, \code{qr.qty}, and related functions) but it is not as
% fully developed as it could be.
%
% Also there is no direct support for sparse matrices in \RR{} although
% \citet{koen:ng:2003} have developed the \pkg{SparseM} package for sparse
% matrices based on SparseKit.
%
% The \pkg{Matrix} package provides S4 classes and methods for dense
% and sparse matrices.  The methods for dense matrices use Lapack and
% BLAS.  The sparse matrix methods use
% CHOLMOD~\citep{Cholmod}, CSparse~\citep{Csparse}
% and other parts (AMD, COLAMD) of Tim Davis' ``SuiteSparse'' collection of
% sparse matrix libraries, many of which also use BLAS.
%
%
%
% \TODO{\Rfun{triu}, \Rfun{tril}, \Rfun{diag}, ...
%   and  \command{as(.,.)} , but of course only when they've seen a few
%   different ones.}
%
% \TODO{matrix operators include \code{\%*\%}, \Rfun{crossprod},
%   \Rfun{tcrossprod}, \Rfun{solve}}
%
% \TODO{\Rfun{expm} is the matrix exponential ... ...}
%
% \TODO{\Rfun{symmpart} and \Rfun{skewpart} compute the symmetric part,
%   \code{(x + t(x))/2} and the skew-symmetric part,
%   \code{(x - t(x))/2} of a matrix \code{x}.}
%
% \TODO{factorizations include \Rfun{Cholesky} (or \Rfun{chol}), \Rfun{lu}, \Rfun{qr} (not yet for dense)}
%
% \TODO{Although generally the result of an operation on dense matrices is
%   a dgeMatrix, certain operations return matrices of special types.}
% \TODO{E.g. show the distinction between \code{t(mm) \%*\% mm}
%   and \code{crossprod(mm)}.}
%
%
% % \bigskip
%
% % ... ... ...  The following is the old \file{Introduction.Rnw} ... FIXME ... ...
%
% \bigskip
%
% \section{Matrix Classes}
% The \pkg{Matrix} package provides classes for real (stored as
% double precision), logical and so-called ``pattern'' (binary) dense and
% sparse matrices. There are provisions to also provide integer and complex
% (stored as double precision complex) matrices.
%
% Note that in \RR, \code{logical} means entries
% \code{TRUE}, \code{FALSE}, or \code{NA}.
% To store just the non-zero pattern for typical sparse matrix algorithms,
% the pattern matrices are \emph{binary}, i.e., conceptually just \code{TRUE}
% or \code{FALSE}.  In \pkg{Matrix}, the pattern matrices all have class
% names starting with \code{"n"} (patter\textbf{n}).
%
% \subsection{Classes for dense matrices}
% \label{ssec:DenseClasses}
%
% For the sake of brevity, we restrict ourselves to the
% \emph{real} (\textbf{d}ouble) classes, but they are paralleled by
% \textbf{l}ogical and patter\textbf{n} matrices for all but the positive
% definite ones.
% \begin{description}
% \item[dgeMatrix] Real matrices in general storage mode
% \item[dsyMatrix] Symmetric real matrices in non-packed storage
% \item[dspMatrix] Symmetric real matrices in packed storage (one triangle only)
% \item[dtrMatrix] Triangular real matrices in non-packed storage
% \item[dtpMatrix] Triangular real matrices in packed storage (triangle only)
% \item[dpoMatrix] Positive semi-definite symmetric real matrices in
%   non-packed storage
% \item[dppMatrix] \ \ ditto \ \ in packed storage
% \end{description}
% Methods for these classes include coercion between these classes, when
% appropriate, and coercion to the \code{matrix} class; methods for
% matrix multiplication (\code{\%*\%}); cross products
% (\code{crossprod}), matrix norm (\code{norm}); reciprocal condition
% number (\code{rcond}); LU factorization (\code{lu}) or, for the
% \code{poMatrix} class, the Cholesky decomposition (\code{chol}); and
% solutions of linear systems of equations (\code{solve}).
%
% %-- mentioned above already:
% % Further, group methods have been defined for the \code{Arith} (basic
% % arithmetic, including with scalar numbers) and the \code{Math} (basic
% % mathematical functions) group..
%
% Whenever a factorization or a decomposition is calculated it is
% preserved as a (list) element in the \code{factors} slot of the
% original object.  In this way a sequence of operations, such as
% determining the condition number of a matrix then solving a linear
% system based on the matrix, do not require multiple factorizations of
% the same matrix nor do they require the user to store the intermediate
% results.
%
% \subsection{Classes for sparse matrices}
% \label{sec:SparseClasses}
%
% Used for large matrices in which most of the elements are known to
% be zero (or \code{FALSE} for logical and binary (``pattern'') matrices).
%
% Sparse matrices are automatically built from \Rfun{Matrix} whenever the
% majority of entries is zero (or \code{FALSE} respectively).  Alternatively,
% \Rfun{sparseMatrix} builds sparse matrices from their non-zero entries and
% is typically recommended to construct large sparse matrices, rather than
% direct calls of \Rfun{new}.
%
% \TODO{E.g. model matrices created from factors with a large number of levels}
%
% \TODO{ or from spline basis functions  (e.g. COBS, package \pkg{cobs}), etc.}
%
% \TODO{Other uses include representations of graphs.
%   indeed; good you mentioned it!
%   particularly since we still have the interface to the \pkg{graph} package.
%   I think I'd like to draw one graph in that article --- maybe the
%   undirected graph corresponding to a crossprod() result of
%   dimension ca. $50^2$}
%
%  \TODO{Specialized algorithms can give substantial savings in amount of
%    storage used and execution time of operations.}
%
%  \TODO{Our implementation is based on the CHOLMOD and CSparse libraries by
%    Tim Davis.}
%
%
%
% \subsection{Representations of sparse matrices}
% \label{ssec:SparseReps}
%
% \subsubsection{Triplet representation (\class{TsparseMatrix})}
% Conceptually, the simplest representation of a sparse matrix is as a
% triplet of an integer vector \code{i} giving the row numbers, an
% integer vector \code{j} giving the column numbers, and a numeric
% vector \code{x} giving the non-zero values in the matrix.\footnote{For efficiency
% reasons, we use ``zero-based'' indexing in the \pkg{Matrix} package, i.e.,
% the row indices \code{i} are in \code{0:(nrow(.)-1)} and the column indices
% \code{j} accordingly.}  In \pkg{Matrix}, the \class{TsparseMatrix} class is the
% virtual class of all sparse matrices in triplet representation.
% Its main use is for easy input or transfer to other classes.
%
% As for the dense matrices, the class of the \code{x} slot may vary, and the
% subclasses may be triangular, symmetric or unspecified (``general''), such that
% the \class{TsparseMatrix} class has several\footnote{the $3 \times 3$
%   actual subclasses of \class{TsparseMatrix} are the three structural
%   kinds, namely \textbf{t}riangular, \textbf{s}ymmetric and \textbf{g}eneral,
%   times three entry classes, \textbf{d}ouble, \textbf{l}ogical, and
%   patter\textbf{n}.}
% `actual'' subclasses,
% the most typical (numeric, general) is \class{dgTMatrix}:
% <<Tsparse-class>>=
% getClass("TsparseMatrix") # (i,j, Dim, Dimnames) slots are common to all
% getClass("dgTMatrix")
% @
% Note that the \emph{order} of the entries in the \code{(i,j,x)} vectors
% does not matter; consequently, such matrices are not unique in their
% representation.  \footnote{
%   Furthermore, there can be \emph{repeated} \code{(i,j)} entries with the
%   customary convention that the corresponding \code{x} entries are
%   \emph{added} to form the matrix element $m_{ij}$.
% }
% %% The triplet representation is row-oriented if elements in the same row
% %% were adjacent and column-oriented if elements in the same column were
% %% adjacent.
%
% \subsubsection{Compressed representations: \class{CsparseMatrix} and \class{RsparseMatrix}}
%
% For most sparse operations we use the compressed column-oriented
% representation (virtual class \class{CsparseMatrix}) (also known as
% ``csc'', ``compressed sparse column'').  Here, instead of storing all
% column indices \code{j}, only the \emph{start} index of every column is stored.
%
% Analogously, there is also a compressed sparse row (csr) representation,
% which e.g. is used in in the \pkg{SparseM} package, and we provide the
% \class{RsparseMatrix} for compatibility and completeness purposes, in
% addition to basic coercion (\code({as(., \textit{<cl>})} between the classes.
% %% (column-oriented triplet) except that \code{i} (\code{j}) just stores
% %% the index of the first element in the row (column).  (There are a
% %% couple of other details but that is the gist of it.)
% These compressed representations remove the redundant row (column)
% indices and provide faster access to a given location in the matrix
% because you only need to check one row (column).
%
% There are certain advantages \footnote{routines can make use of
%   high-level (``level-3'') BLAS in certain sparse matrix computations}
% to csc in systems like \RR{}, Octave and Matlab where dense matrices are
% stored in column-major order, therefore it is used in sparse matrix
% libraries such as CHOLMOD or CSparse of which we make use.  For this
% reason, the \class{CsparseMatrix} class and subclasses are the
% principal classes for sparse matrices in the \pkg{Matrix} package.
%
%
% The Matrix package provides the following classes for sparse matrices
% \FIXME{many more --- maybe explain naming scheme?}
% \begin{description}
% \item[dgTMatrix] general, numeric, sparse matrices in (a possibly
%   redundant) triplet form.  This can be a convenient form in which to
%   construct sparse matrices.
% \item[dgCMatrix] general, numeric, sparse matrices in the (sorted) compressed
%   sparse column format.
% \item[dsCMatrix] symmetric, real, sparse matrices in the (sorted)
%   compressed sparse column format.  Only the upper or the lower triangle is
%   stored.  Although there is provision for both forms, the lower
%   triangle form works best with TAUCS.
% \item[dtCMatrix] triangular, real, sparse matrices in the (sorted)
%   compressed sparse column format.
% \end{description}
%
% \TODO{Can also read and write the Matrix Market and read the Harwell-Boeing
%   representations.}
%
% \TODO{Can convert from a dense matrix to a sparse matrix (or use the
%   Matrix function) but going through an intermediate dense matrix may
%   cause problems with the amount of memory required.}
%
% \TODO{similar range of operations as for the dense matrix classes.}
%
%
% \section{More detailed examples of ``Matrix'' operations}
%
% Have seen \texttt{drop0()} above, %(p.3); only with logical
% showe a nice double example (where you see ``.'' and ``0'').
%
% Show the use of \code{dim<-} for \emph{resizing} a (sparse) matrix.
%
% Maybe mention  \Rfun{nearPD}.
%
% \TODO{Solve a sparse least squares problem and demonstrate memory / speed gain}
%
% \TODO{mention \code{lme4} and \Rfun{lmer}, maybe use one example to show the
%   matrix sizes.}
%
%
% \section{Notes about S4 classes and methods implementation}
%
% Maybe we could % even here (for R News, not only for JSS)
% give   some glimpses of implementations at least on the \RR{} level ones?
%
%  \TODO{The class hierarchy: a non-trivial tree where only the leaves
%     are ``actual'' classes.}
%
%  \TODO{The main advantage of the multi-level hierarchy is that
%     methods can often be defined on a higher (virtual class) level
%     which ensures consistency [and saves from ``cut \& paste'' and
%     forgetting things]}
%
%  \TODO{Using Group Methods}
%
%
% \section{Session Info}
%
% <<sessionInfo, results=tex>>=
% toLatex(sessionInfo())
% @
%
% \bibliography{Matrix}
\bibliography{mybibliographybibtex}
\end{document}
